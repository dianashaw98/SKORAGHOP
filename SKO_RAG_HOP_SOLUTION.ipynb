{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "qycoxzexgwpq3nddzhxj",
   "authorId": "154296475017",
   "authorName": "DSHAW_SFC",
   "authorEmail": "diana.shaw@snowflake.com",
   "sessionId": "f5e7ba7d-29af-4e98-bf4a-92ab93fb41de",
   "lastEditTime": 1737667775807
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "name": "Title",
    "collapsed": false,
    "resultHeight": 311
   },
   "source": "# ❄️ SKO RAG HOP - Snowflake Cortex with Anthropic and LLM Observability ❄️\n\nThis notebook demonstrates how to create a Retrieval-Augmented Generation (RAG) workflow in Snowflake using Cortex Search Services, integrate Anthropic LLMs like Claude 2.5, and evaluate responses with new LLM Observability features. Below is an overview of the flow and its key components.\n\n### Step 1: Parse and Chunk Text from PDFs (BUILD)\n### Step 2: Create Cortex Search Service (RETRIEVE)\n### Step 3: Test Search Results with Experimental Configurations (AUGMENT)\n### Step 4: Pass Retrieved Content to LLMs (GENERATE)\n### Step 5: Create RAG Application Class (SERVE)\n### Step 6: Observe and Evaluate LLM Performance with AI Observabillity (EVALUATE)"
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "python",
    "name": "Flow",
    "collapsed": false,
    "codeCollapsed": true,
    "resultHeight": 87
   },
   "outputs": [],
   "source": "# Import necessary functions\nimport streamlit as st\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\n# Define image in a stage and read the file\nimage=session.file.get_stream(\"@SKO.HOP.RAG/Flow.jpg\", decompress=False).read() \n\n# Display the image\nst.image(image, width=800)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "Libraries",
    "collapsed": false,
    "resultHeight": 0,
    "codeCollapsed": false
   },
   "source": "import snowflake.snowpark as snowpark\n\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "64842e12-1e4c-423b-a568-376102123485",
   "metadata": {
    "language": "sql",
    "name": "ViewStage",
    "collapsed": false,
    "resultHeight": 426,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- List files in the stage to identify PDFs\nLS @SKO.HOP.RAG;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d25663e4-08a0-418c-a6d4-96e913aef549",
   "metadata": {
    "name": "Step1",
    "collapsed": false
   },
   "source": "## Step 1: Parse and Chunk Text from PDFs\nWe begin by parsing the content of uploaded PDFs and chunking the text using Snowflake's [PARSED_TEXT](https://docs.snowflake.com/sql-reference/functions/parse_document-snowflake-cortex) and [SPLIT_TEXT_RECURSIVE_CHARACTER](https://docs.snowflake.com/sql-reference/functions/split_text_recursive_character-snowflake-cortex) features. These steps structure the text into manageable segments optimized for retrieval. To ensure that the PDF parsing and chunking have been processed correctly, we run queries on the parsed and chunked tables. This step helps verify the integrity of the content.\n\nObjective: **Transform unstructured content into indexed chunks for efficient search and retrieval.**\n\nKey Outputs:\n- SKO.HOP.PARSED_TEXT: Table containing the raw text.\n- SKO.HOP.CORTEX_CHUNK: Chunked, searchable content."
  },
  {
   "cell_type": "code",
   "id": "3f293bf7-06b5-4d05-9666-ad3096d25a31",
   "metadata": {
    "language": "sql",
    "name": "CreateParsedTextTable",
    "collapsed": false,
    "resultHeight": 111,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- Create a table to hold the extracted text from the PDF files loaded in the SKO.HOP.RAG stage\n\n-- Complete the missing code (???) to use create a table called PARSED_TEXT\n\nCREATE OR REPLACE TABLE SKO.HOP.PARSED_TEXT (relative_path VARCHAR(500), raw_text VARIANT);",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "026e7c0b-6c8f-472f-9cfe-94e1351b9925",
   "metadata": {
    "language": "sql",
    "name": "UseParseDocument",
    "collapsed": false,
    "codeCollapsed": false,
    "resultHeight": 111
   },
   "outputs": [],
   "source": "-- Use Snowflake's new PARSED_TEXT feature to extract the text from the PDFs loaded in @SKO.HOP.RAG stage\n-- Cortex PARSE_DOCUMENT documentation link is https://docs.snowflake.com/sql-reference/functions/parse_document-snowflake-cortex\n\n-- Complete the missing code (???) to:\n---- Insert into your newly created PARSED_TEXT table\n---- Use Cortex PARSE_DOCUMENT feature and ocr mode\n\nINSERT INTO SKO.HOP.PARSED_TEXT (relative_path, raw_text)\nWITH pdf_files AS (\n    SELECT DISTINCT\n        METADATA$FILENAME AS relative_path\n    FROM @SKO.HOP.RAG\n    WHERE METADATA$FILENAME ILIKE '%.pdf'\n      -- Exclude files that have already been parsed\n      AND METADATA$FILENAME NOT IN (SELECT relative_path FROM PARSED_TEXT)\n)\nSELECT \n    relative_path,\n    SNOWFLAKE.CORTEX.PARSE_DOCUMENT(\n        '@SKO.HOP.RAG',  -- Your stage name\n        relative_path,  -- File path\n        {'mode': 'ocr'}  -- Adjust mode as needed ('layout', 'ocr')\n    ) AS raw_text\nFROM pdf_files;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0dce5839-f0eb-4d4f-838e-9b3633979c8c",
   "metadata": {
    "language": "sql",
    "name": "TestingSplitTextFunction",
    "collapsed": false,
    "resultHeight": 111
   },
   "outputs": [],
   "source": "-- Use Snowflake's new SPLIT_TEXT_RECURSIVE_CHARACTER feature to chunk parsed text from the PDFs loaded in @SKO.HOP.RAG stage\n-- Cortex SPLIT_TEXT_RECURSIVE_CHARACTER documentation link is https://docs.snowflake.com/sql-reference/functions/split_text_recursive_character-snowflake-cortex\n\n-- Complete the missing code (???) to:\n---- Create a new table called CORTEX_CHUNK to hold the chunked text from your PDF documents\n---- Use Cortex SPLIT_TEXT_RECURSIVE_CHARACTER feature with a 800 chunk size and 100 overlap size\n\nCREATE OR REPLACE TABLE SKO.HOP.CORTEX_CHUNK AS\nWITH text_chunks AS (\n    SELECT\n        relative_path,\n        SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER(\n            raw_text:content::STRING,  -- Extract the 'content' field from the JSON\n            'none',    -- Adjust to 'markdown' if needed\n            800,       -- Adjust chunk size\n            100,       -- Adjust overlap size\n            ['\\n']     -- Adjust separators\n        ) AS chunks\n    FROM SKO.HOP.PARSED_TEXT\n)\nSELECT\n    relative_path,\n    c.value AS chunk  -- Extract each chunk of the parsed text\nFROM text_chunks,\nLATERAL FLATTEN(INPUT => chunks) c;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c4fb6cd6-5e5b-4255-a00a-7099fb4d37f0",
   "metadata": {
    "language": "python",
    "name": "PDFName",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# Select a PDF file to view\nPDF_name = 'Arctic Embed Multilingual.pdf'",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "496d3cb3-620b-4d12-a655-7bf71c36bdee",
   "metadata": {
    "language": "sql",
    "name": "ViewParsedText",
    "collapsed": false,
    "resultHeight": 111,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- check the RAW_TEXT to ensure the PDF was parsed as expected\n-- Complete the missing code (???) to check the RAW_TEXT to ensure the PDF was parsed as expected\n\nSELECT RELATIVE_PATH, RAW_TEXT \nFROM SKO.HOP.PARSED_TEXT \nWHERE RELATIVE_PATH ILIKE '%' || '{{PDF_name}}' || '%';",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b64cfb61-0168-43ea-9a70-8242dc45ec07",
   "metadata": {
    "language": "sql",
    "name": "ViewChunkedText",
    "collapsed": false,
    "resultHeight": 438
   },
   "outputs": [],
   "source": "-- check the CORTEX_CHUNK to ensure the PDF was chunked as expected\n-- Complete the missing code (???) to check the CORTEX_CHUNK to ensure the PDF was chunked as expected\n\nSELECT * \nFROM SKO.HOP.CORTEX_CHUNK \nWHERE RELATIVE_PATH ILIKE '%' || '{{PDF_name}}' || '%';",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1856f350-86ea-42e9-baf1-5da4165e73ef",
   "metadata": {
    "name": "Step2",
    "collapsed": false
   },
   "source": "## Step 2: Create Cortex Search Service\nNext, we create a [Cortex Search Service](https://docs.snowflake.com/LIMITEDACCESS/cortex-search/cortex-search-overview#overview) that enables retrieval of relevant text chunks for any query. This service uses the CHUNK column from the chunked table as the indexed content.\n\nPurpose: **Index and search chunked content to support the RAG pipeline.**\n\nCommand:\n```sql\nCREATE OR REPLACE CORTEX SEARCH SERVICE SKO.HOP.RAG_SEARCH_SERVICE ON SEARCH_COL WAREHOUSE = COMPUTE_WH TARGET_LAG = '1 day' AS SELECT  ...;\n```"
  },
  {
   "cell_type": "code",
   "id": "8fe16397-d999-4901-b434-43681a0579b8",
   "metadata": {
    "language": "python",
    "name": "CortexSearch",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "# Define image in a stage and read the file\nimage=session.file.get_stream(\"@SKO.HOP.RAG/CortexSearch.jpg\", decompress=False).read() \nst.image(image, width=800)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "44a330f9-c774-47f2-b8ca-031ae441c602",
   "metadata": {
    "language": "sql",
    "name": "CreateCortexSearchService",
    "collapsed": false,
    "resultHeight": 111
   },
   "outputs": [],
   "source": "-- Create a search service over your new chunked pdf table that has one searchable text\n-- Cortex Search Service documentation link is https://docs.snowflake.com/LIMITEDACCESS/cortex-search/cortex-search-overview#overview\n\n-- Complete the missing code (???) to:\n---- Create a search service called SKO.HOP.RAG_SEARCH_SERVICE to run over your new chunked pdf table\n---- Queries to the service will search on a new column called SEARCH_COL \n---- Use an x-small warehouse\n---- Use a target_lag of 365 days\n---- SEARCH_COL is the name of the concatenation of RELATIVE_PATH and CHUNK from the CORTEX_CHUNK table\n\nCREATE OR REPLACE CORTEX SEARCH SERVICE SKO.HOP.RAG_SEARCH_SERVICE\n    ON SEARCH_COL\n    WAREHOUSE = COMPUTE_WH\n    TARGET_LAG = '365 days'\n    AS SELECT \n        RELATIVE_PATH,\n        CHUNK,\n    (RELATIVE_PATH || ' ' || CHUNK) AS SEARCH_COL\nFROM SKO.HOP.CORTEX_CHUNK;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "348e3271-b238-465f-995c-3c8f1df0cf2b",
   "metadata": {
    "name": "Step3",
    "collapsed": false
   },
   "source": "## Step 3: Test Search Results with Experimental Configurations\nWe will now evaluate [Snowflake Cortex Experimental Knobs](https://docs.google.com/document/d/1HkHtDiY3CmzpSewCe_s9fpMNE5spOUvNSwr6CxFerqE/edit?usp=sharing) to fine-tune the retrieval service and analyze confidence scores and result rankings across configurations. These tests focus on boosting, recency, headers, and reranking to optimize search relevance.\n\n**Configurations Tested:**\n- **Boosted vs. Unboosted:** Compare the impact of keyword emphasis on rankings and scores.\n- **Time-Based Decays:** Test how prioritizing recent documents affects relevance.\n- **Header Boosts:** Evaluate the influence of structured headers (e.g., Markdown) on ranking.\n- **Reranked vs. Non-Reranked:** Analyze trade-offs between query latency and search quality.\n\n**Key Metrics:**\n- **Confidence Scores:** Global relevance scores (0–3) for each result.\n- **Result Rankings:** Position changes reveal the effectiveness of configurations.\n\nBy testing these configurations, we aim to enhance Cortex Search Service performance for specific use cases."
  },
  {
   "cell_type": "code",
   "id": "53e406ce-4f72-4fd5-8360-fc85e10c73bc",
   "metadata": {
    "language": "python",
    "name": "CortexSearchEnhancements",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "# Define image in a stage and read the file\nimage=session.file.get_stream(\"@SKO.HOP.RAG/CortexSearchEnhancements.jpg\", decompress=False).read() \nst.image(image, width=800)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a7c8762e-0ebb-4aa7-8a2a-d4af9cbba828",
   "metadata": {
    "language": "sql",
    "name": "CombineBoostedUnboostedResults",
    "collapsed": false
   },
   "outputs": [],
   "source": "-- This query compares Cortex Search Service results across multiple experimental settings: boosted (using softBoosts), unboosted (default settings), and decayed (time-based decay).\n-- The results are presented side-by-side to analyze the impact of each configuration on confidence scores and document ranking for matching search columns.\n-- This analysis helps evaluate the effectiveness of boosting and decay strategies in improving search relevance and recency-based ranking.\n\n-- Complete the missing code (???) to:\n---- Call your SKO.HOP.RAG_SEARCH_SERVICE to test experimental configurations\n---- Use the query \"How does Cortex support multilingual queries?\" \n---- For the boosted_results section of the query, use the phrase \"Arctic Embed Multilingual.pdf\"\n---- Set returnConfidenceScores to true for all\n\nWITH boosted_results AS (\n    SELECT DISTINCT\n        VALUE:\"SEARCH_COL\"::STRING AS SearchColumn,\n        VALUE:\"@CONFIDENCE_SCORE\"::STRING AS ConfidenceScore\n    FROM (\n        SELECT PARSE_JSON(\n            SNOWFLAKE.CORTEX.SEARCH_PREVIEW(\n                'SKO.HOP.RAG_SEARCH_SERVICE',\n                '{\n                    \"query\": \"How does Cortex support multilingual queries?\",\n                    \"experimental\": {\n                        \"softBoosts\": [\n                            { \"phrase\": \"Arctic Embed Multilingual.pdf\" }\n                        ],\n                        \"returnConfidenceScores\": true\n                    }\n                }'\n            )\n        ) AS boosted_json\n    ),\n    LATERAL FLATTEN(input => boosted_json:\"results\")\n),\nunboosted_results AS (\n    SELECT DISTINCT\n        VALUE:\"SEARCH_COL\"::STRING AS SearchColumn,\n        VALUE:\"@CONFIDENCE_SCORE\"::STRING AS ConfidenceScore\n    FROM (\n        SELECT PARSE_JSON(\n            SNOWFLAKE.CORTEX.SEARCH_PREVIEW(\n                'SKO.HOP.RAG_SEARCH_SERVICE',\n                '{\n                    \"query\": \"How does Cortex support multilingual queries?\",\n                    \"experimental\": {\n                        \"returnConfidenceScores\": true\n                    }\n                }'\n            )\n        ) AS unboosted_json\n    ),\n    LATERAL FLATTEN(input => unboosted_json:\"results\")\n),\ndecayed_results AS (\n    SELECT DISTINCT\n        VALUE:\"SEARCH_COL\"::STRING AS SearchColumn,\n        VALUE:\"@CONFIDENCE_SCORE\"::STRING AS ConfidenceScore\n    FROM (\n        SELECT PARSE_JSON(\n            SNOWFLAKE.CORTEX.SEARCH_PREVIEW(\n                'SKO.HOP.RAG_SEARCH_SERVICE',\n                '{\n                    \"query\": \"How does Cortex support multilingual queries?\",\n                    \"experimental\": {\n                        \"decays\": {\n                            \"last_modified\": {\n                                \"weight\": 1.0,\n                                \"limitHours\": 240.0\n                            }\n                        },\n                        \"returnConfidenceScores\": true\n                    }\n                }'\n            )\n        ) AS decayed_json\n    ),\n    LATERAL FLATTEN(input => decayed_json:\"results\")\n)\nSELECT \n    COALESCE(b.SearchColumn, u.SearchColumn, d.SearchColumn) AS SearchColumn,\n    b.ConfidenceScore AS BoostedConfidenceScore,\n    u.ConfidenceScore AS UnboostedConfidenceScore,\n    d.ConfidenceScore AS DecayedConfidenceScore\nFROM\n    boosted_results b\nFULL OUTER JOIN unboosted_results u\n    ON b.SearchColumn = u.SearchColumn\nFULL OUTER JOIN decayed_results d\n    ON COALESCE(b.SearchColumn, u.SearchColumn) = d.SearchColumn;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ffe09839-2bd2-4b47-8a00-ae76967d3bd1",
   "metadata": {
    "language": "sql",
    "name": "TestHeaderBoost",
    "collapsed": false
   },
   "outputs": [],
   "source": "-- Query compares Cortex Search Service results using header boosts\n-- Analyzes the impact of prioritizing header matches on document ranking and confidence scores\n\n-- Complete the missing code (???) to:\n---- Call your SKO.HOP.RAG_SEARCH_SERVICE to test experimental configurations\n---- Use the query \"How does Cortex support multilingual queries?\" \n---- Use header boost multiplier of 2.0 to double the importance of header matches in comparison to body text matches\n---- Set returnConfidenceScores to true\n\nWITH header_boost_results AS (\n    SELECT DISTINCT\n        VALUE:\"SEARCH_COL\"::STRING AS SearchColumn,\n        VALUE:\"@CONFIDENCE_SCORE\"::STRING AS ConfidenceScore\n    FROM (\n        SELECT PARSE_JSON(\n            SNOWFLAKE.CORTEX.SEARCH_PREVIEW(\n                'SKO.HOP.RAG_SEARCH_SERVICE',\n                '{\n                    \"query\": \"How does Cortex support multilingual queries?\",\n                    \"experimental\": {\n                        \"retrievalWeights\": {\n                            \"headerBoost\": {\n                                \"multiplier\": 2.0,\n                                \"skipStopWords\": true\n                            }\n                        },\n                        \"returnConfidenceScores\": true\n                    }\n                }'\n            )\n        ) AS header_boost_json\n    ),\n    LATERAL FLATTEN(input => header_boost_json:\"results\")\n)\nSELECT \n    SearchColumn,\n    ConfidenceScore AS HeaderBoostConfidenceScore\nFROM\n    header_boost_results;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2859c78c-4bd3-4bf6-b355-68f357830b74",
   "metadata": {
    "name": "Step4",
    "collapsed": false
   },
   "source": "## Step 4: Pass Retrieved Content to LLMs\nThis step demonstrates how to pass retrieved contextual content to various LLMs using the Snowflake Cortex [`COMPLETE`](https://docs.snowflake.com/en/sql-reference/functions/complete-snowflake-cortex) function. The process includes:\n\n- **Retrieving Contextual Information**: Context is fetched from the search service.\n- **Generating Structured Prompts**: The retrieved context is injected into prompts for LLMs.\n- **LLM Interaction**: Prompts are passed to models like `mistral-7b`, `mistral-large2`, and `Anthropic Claude 3.5` for response generation.\n- **Comparative Analysis**: Model outputs are compared for quality, relevance, and coherence.\n\nExample Query:\n```sql\nSELECT SNOWFLAKE.CORTEX.COMPLETE(\n    'claude-3-5-sonnet',\n    CONCAT('Your context: ', (SELECT LISTAGG(CHUNK, ' ') FROM searchresults))\n) AS RESPONSE\nFROM searchresults;\n```"
  },
  {
   "cell_type": "code",
   "id": "bd3c541d-c305-47f4-96c5-64c7c8dfba92",
   "metadata": {
    "language": "python",
    "name": "AddCortexComplete",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "# Define image in a stage and read the file\nimage=session.file.get_stream(\"@SKO.HOP.RAG/CortexSearch_Complete.jpg\", decompress=False).read() \nst.image(image, width=800)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "605f5809-2ca9-4908-9d1a-1ad6fba7db29",
   "metadata": {
    "name": "QueryIdeas",
    "collapsed": false,
    "resultHeight": 169
   },
   "source": "**Queries to test the capabilities of the LLMs based on the PDF content:**\n- How does Snowflake simplify the deployment of retrieval-augmented generation (RAG) workflows?\n- How does Snowflake's Arctic Embed 2.0 enhance multilingual search capabilities while maintaining efficiency and quality?\n- How does Snowflake's FeatEng benchmark enhance the evaluation of large language models (LLMs) in data science tasks?\n- How does Snowflake's model hotswapping enhance the efficiency and scalability of LLM inference?"
  },
  {
   "cell_type": "code",
   "id": "b19284d0-107c-4c7f-aa2e-5378159c4a3b",
   "metadata": {
    "language": "python",
    "name": "CheckSearchResults",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Query your Snowflake Cortex Search Service using the Snowpark Python API to retrieve and process search results.\n\n# Complete the missing code (???) to:\n## Specify your database 'SKO', your schema 'HOP', and your Cortex Search Service named 'RAG_SEARCH_SERVICE'\n## Specify your SEARCH_COL as the column of interest\n\nfrom snowflake.snowpark import Session\nfrom snowflake.core import Root\nroot = Root(session)\n\ntranscript_search_service = (root\n  .databases['SKO']\n  .schemas['HOP']\n  .cortex_search_services['RAG_SEARCH_SERVICE']\n)\n\nresp = transcript_search_service.search(\n  query=\"\"\"How does Snowflake simplify the deployment of retrieval-augmented generation (RAG) workflows?\"\"\",\n  columns=['SEARCH_COL'],\n  limit=3\n)\nresults = resp.results\n\ncontext_str = \"\"\nfor i, r in enumerate(results):\n    context_str += f\"Context document {i+1}: {r['SEARCH_COL']}\\n****************\\n\"\n\nprint(context_str)\ndf = session.create_dataframe(resp.results)\ndf.create_or_replace_temp_view(\"searchresults\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f80f8035-e606-4503-b2cc-3e7b32035cc3",
   "metadata": {
    "language": "python",
    "name": "AnthropicClaudeSonnetOverview",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "# Define image in a stage and read the file\nimage=session.file.get_stream(\"@SKO.HOP.RAG/Claude35Sonnet.jpg\", decompress=False).read() \n\n# Display the image\nst.image(image, width=800)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6e31d76d-1106-4527-a059-9cf407fb0a84",
   "metadata": {
    "language": "sql",
    "name": "Test3LLMs",
    "collapsed": false,
    "resultHeight": 111,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- Create a temporary table with the LLM responses\n\n-- Complete the missing code (???) to:\n---- Create a TEMPORARY table called LLMResults\n---- Use mistral-7b for the MISTRAL_7B (first model)\n---- Use mistral-large2 for the MISTRAL_LARGE2 (second model)\n---- Use the Anthropic model (claude-3-5-sonnet) for the CLAUDE_35 (third model)\n\nCREATE OR REPLACE TEMPORARY TABLE LLMResults AS\nWITH PROMPT_TEXT AS (\n  SELECT CONCAT(\n    'You are a helpful AI assistant specialized in assisting Sales Engineers...',\n    (SELECT LISTAGG(SEARCH_COL, ' ') FROM searchresults),\n    ' Focus on key points and avoid unnecessary details.'\n  ) AS P\n)\nSELECT \n   SNOWFLAKE.CORTEX.COMPLETE('mistral-7b', (SELECT P FROM PROMPT_TEXT)) AS MISTRAL_7B,\n   SNOWFLAKE.CORTEX.COMPLETE('mistral-large2', (SELECT P FROM PROMPT_TEXT)) AS MISTRAL_LARGE2,\n   SNOWFLAKE.CORTEX.COMPLETE('claude-3-5-sonnet', (SELECT P FROM PROMPT_TEXT)) AS CLAUDE_35;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4f90ca06-7a03-4d4a-bb81-a6d751c8bc76",
   "metadata": {
    "language": "python",
    "name": "ViewMistral_7BResults",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "df = session.sql(\"SELECT * FROM LLMResults\").to_pandas()\nst.subheader(\"Output for Mistral-7b LLM\")\nmistral_7b_value = df.iloc[0][\"MISTRAL_7B\"]\nst.code(mistral_7b_value, language=\"text\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "56f83f15-32a5-4314-bd54-dc5ab4b55089",
   "metadata": {
    "language": "python",
    "name": "ViewMistralLarge2Result",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "st.subheader(\"Output for Mistral-Large2 LLM\")\nmistral_7b_value = df.iloc[0][\"MISTRAL_LARGE2\"]\nst.code(mistral_7b_value, language=\"text\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "87a7ccc8-1f91-4d9f-af6c-cfc53de0a519",
   "metadata": {
    "language": "python",
    "name": "ViewAnthropicResult",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "st.subheader(\"Output for Anthropic Claude 3.5 Sonnet LLM\")\nclaude_rag = df.iloc[0][\"CLAUDE_35\"]\nst.code(claude_rag, language=\"text\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "08d45331-c62b-4bb5-b947-77971382483a",
   "metadata": {
    "name": "Step5",
    "collapsed": false
   },
   "source": "## Step 5: Create RAG Application Classes\n\nIn this step, we will create two Python classes to build a Retrieval-Augmented Generation (RAG) pipeline:\n\n1. **`CortexSearchRetriever`**:\n   - This class interacts with the Cortex Search Service to retrieve relevant contextual information based on a user query.\n   - It connects to the Cortex Search Service using Snowflake's `Root` object and performs a search with the specified query and result limit.\n   - The retrieved context (a list of relevant chunks) will be used to generate prompts for LLMs.\n\n2. **`RAGWithObservability`**:\n   - This class integrates the retrieval functionality with a specified Large Language Model (LLM) to complete the RAG pipeline.\n   - It uses the retriever to fetch context, creates a structured prompt by combining the context with the user query, and generates a response using the Snowflake Cortex `COMPLETE` function.\n   - The class allows testing of different LLMs (e.g., `llama3.1-8b`, `mistral-7b`, `claude-3-5-sonnet`) by specifying the desired model.\n\n### Workflow Summary:\n1. The `CortexSearchRetriever` retrieves relevant context from the Cortex Search Service.\n2. The `RAGWithObservability` uses this context to create prompts and generate responses with the specified LLM.\n\nThese two classes work together to streamline the RAG pipeline, enabling efficient retrieval and response generation for various use cases."
  },
  {
   "cell_type": "code",
   "id": "65daf34a-f360-4223-bff2-60f016ff6c3e",
   "metadata": {
    "language": "python",
    "name": "CortexSearchRetriever",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Define the retriever class for interacting with the Cortex Search Service\n\n# Complete the missing code (???) to:\n## Specify your database 'SKO', your schema 'HOP', and your Cortex Search Service named 'RAG_SEARCH_SERVICE'\n## Specify your SEARCH_COL as the column of interest\n## Intialize retriever with your CortexSearchRetriever class\n## Use \"What are some components of the Snowflake Cortex offering?\" for the test_query\n\nfrom typing import List\nfrom snowflake.snowpark import Session\nfrom snowflake.core import Root\n\n# CortexSearchRetriever\nclass CortexSearchRetriever:\n    def __init__(self, session: Session, limit_to_retrieve: int = 4):\n        self._session = session\n        self._limit_to_retrieve = limit_to_retrieve\n        \n\n    def retrieve(self, query: str) -> List[str]:\n        root = Root(session)\n        cortex_search_service = (\n            root\n            .databases[\"SKO\"]\n            .schemas[\"HOP\"]\n            .cortex_search_services[\"RAG_SEARCH_SERVICE\"]\n        )\n        resp = cortex_search_service.search(\n            query=query,\n            columns=[\"SEARCH_COL\"],\n            limit=self._limit_to_retrieve,\n        )\n        return [row[\"SEARCH_COL\"] for row in resp.results] if resp.results else []\n\n# Initialize the retriever\nretriever = CortexSearchRetriever(session=session, limit_to_retrieve=5)\ntest_query = \"What are some components of the Snowflake Cortex offering?\"\nretrieved_context = retriever.retrieve(query=test_query)\nprint(retrieved_context)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e3b51fe4-118c-437c-a348-462d2fb25f58",
   "metadata": {
    "language": "python",
    "name": "DefineRAGClass",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Create the RAGWithObservability class to structure the RAG pipeline\nfrom snowflake.cortex import Complete\nfrom trulens.apps.custom import instrument\n\n\nclass RAGWithObservability():\n    def __init__(self, llm_model, retriever):\n        self.llm_model = llm_model\n        self.retriever = retriever\n        # self.retriever = CortexSearchRetriever(session=session, limit_to_retrieve=4)\n#Here we're using the @instrument decorator to indicate to trulens that we want to trace this step of the applicaiton\n    @instrument\n    def retrieve_context(self, query: str) -> List[str]:\n        return self.retriever.retrieve(query)\n\n    @instrument\n    def create_prompt(self, query: str) -> str:\n        retrieved_context = self.retrieve_context(query)\n        prompt = f\"\"\"\n        You are an expert assistant extracting information from context provided.\n        Answer the question based on the context. Be concise and do not hallucinate.\n        If you don't have the information, just say so.\n        Context: {' '.join(retrieved_context)}\n        Question: {query}\n        Answer:\n        \"\"\"\n        return prompt, retrieved_context\n\n    @instrument\n    def query(self, query: str):\n    \n        prompt, retrieved_context =self.create_prompt(query)\n        \n        df_response = Complete(self.llm_model, prompt)\n        return df_response\n\n\n#Define LLM classes\nllama_rag = RAGWithObservability('llama3.1-8b', retriever)\nmistral7b_rag = RAGWithObservability('mistral-7b', retriever)\nclaude_rag = RAGWithObservability('claude-3-5-sonnet', retriever)\n\n#Get responses\nllama_response = llama_rag.query(test_query)\nmistral_response = mistral7b_rag.query(test_query)\nclaude_response = claude_rag.query(test_query)\n\n# Print responses\nprint(f\"Query: {test_query}\")\nprint(f\"Llama response -  {llama_response}\")\nprint(f\"Mistral-7b response - {mistral_response}\")\nprint(f\"Claude response -  {llama_response}\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d5b33b25-20c3-4cd7-9dce-118400be593c",
   "metadata": {
    "name": "Step6",
    "collapsed": false
   },
   "source": "## Step 6: Observe and Evaluate LLM Performance with AI Observabillity (Trulens)\n\n**Defining observability parameters for LLM performance using Trulens**\nFrom this step, we enhance the Retrieval-Augmented Generation (RAG) process by introducing observability. Observability ensures that LLM responses can be measured and evaluated based on various feedback metrics, providing insights into the model's performance and areas for improvement.\n\n**How TruSession Connects to Snowflake**\nThe `TruSession` object establishes a connection between Trulens and Snowflake using the `SnowflakeConnector`. This connection enables Trulens to access Snowflake Cortex data and evaluate LLM performance based on **defining feedback metrics** like:\n- **Answer Relevance** - how relevant is the response from the model to the user's prompt?\n- **Context Relevance** - how relevant is the retrieved context to the user's prompt?\n- **Groundedness**  - how well grounded in the retrieved context is the LLM's response? \n- **Concisenss** - how concise is the LLM's response?\n- **Coherance** - how coherent is the LLM's response?\n#### Note that these are just a few of the possible feedback functions we could use for evaluation here, see [docs](https://www.trulens.org/getting_started/core_concepts/rag_triad/#putting-it-together) for more detail on these feedback functions."
  },
  {
   "cell_type": "code",
   "id": "6d13d6db-55d3-4841-8915-1088cab39a45",
   "metadata": {
    "language": "python",
    "name": "AIObservability",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "# Define image in a stage and read the file\nimage=session.file.get_stream(\"@SKO.HOP.RAG/AIObservability.jpg\", decompress=False).read() \n\n# Display the image\nst.image(image, width=800)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6b58d853-417c-4d46-abfd-6c928613a45f",
   "metadata": {
    "language": "python",
    "name": "LLMObservabilitySetup",
    "collapsed": false
   },
   "outputs": [],
   "source": "from trulens.core import TruSession\nfrom trulens.connectors.snowflake import SnowflakeConnector\ntru_snowflake_connector = SnowflakeConnector(snowpark_session=session)\n\n# Initialize TruLens session\ntru_session = TruSession(connector=tru_snowflake_connector)\n\n# Confirmation message\nprint(\"TruLens session successfully initialized.\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "32a8d565-6112-42a7-938b-a465751f7466",
   "metadata": {
    "language": "python",
    "name": "DefineFeedbackFunctions",
    "collapsed": false,
    "resultHeight": 150,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Set up feedback functions to evaluate the model's performance.\nfrom trulens.providers.cortex.provider import Cortex\nfrom trulens.core import Feedback, SnowflakeFeedback, Select\nimport numpy as np\nfrom functools import partial \n\n# Use llama3.1-8b as the LLM that executes evaluations\nprovider = Cortex(session, \"llama3.1-8b\")\n\n# Answer relevance - how relevant is the response to the prompt?\nf_answer_relevance = (\n    Feedback(provider.relevance_with_cot_reasons, name=\"Answer Relevance\")\n    .on(Select.RecordInput.collect())  # Use `collect()` for Trulens\n    .on(Select.RecordOutput.collect())  # Use `collect()` for Trulens\n    .aggregate(np.mean)\n)\n\n# Context relevance - how relevant is the retrieved context to the prompt?\nf_context_relevance = (\n    Feedback(provider.context_relevance, name=\"Context Relevance\")\n    .on(Select.RecordInput.collect())  # Use `collect()` for Trulens\n    .on(Select.RecordCalls.retrieve_context.rets.collect())  # Use `collect()` for Trulens\n    .aggregate(np.mean)\n)\n\n# Groundedness - how well-grounded in the retrieved context is the response?\nf_groundedness = (\n    Feedback(\n        partial(provider.groundedness_measure_with_cot_reasons, use_sent_tokenize=False), \n        name=\"Groundedness\", \n        use_sent_tokenize=False\n    )\n    .on(Select.RecordCalls.retrieve_context.rets.collect())  # Use `collect()`\n    .on_output()\n)\n\n# Conciseness - how concise is the LLM's response?\nf_conciseness = Feedback(provider.conciseness, name=\"Conciseness\").on_output()\n\n# Coherence - how coherent is the LLM's response?\nf_coherence = Feedback(provider.coherence, name=\"Coherence\").on_output()\n\n# Combine all feedback functions into a list\nfeedbacks = [f_context_relevance, f_groundedness, f_answer_relevance, f_conciseness, f_coherence]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "853a41cd-96db-44e8-91e4-0b1109cd79b6",
   "metadata": {
    "language": "python",
    "name": "DefineTestPrompts",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Test Prompts\nprompts = [\n    \"What are some pitfalls of gen AI workloads?\",\n    \"What languages can I use to embed text into vectors in snowflake?\",\n    \"How does Snowflake simplify the deployment of retrieval-augmented generation (RAG) workflows?\",\n    \"How does a multi-turn conversation with cortex analyst work?\",\n    \"How does neuroscience influence AI architectures?\",\n    \"Who is an expert on Doc AI at Snowflake?\",\n    \"Can I use cortex analyst on multiple tables in a star schema?\",\n    \"What are some common LLM benchmark tests\"\n]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f1666859-0a22-461a-9fe5-aa03a4df6a11",
   "metadata": {
    "language": "python",
    "name": "TestRARObservability",
    "collapsed": false,
    "resultHeight": 0,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from trulens.apps.custom import TruCustomApp\n\n# Test prompts\nprompts = [\n    \"What are some components of the Snowflake Cortex offering?\",\n    \"How does Snowflake simplify the deployment of retrieval-augmented generation workflows?\",\n    \"How does a multi-turn conversation with cortex analyst work?\",\n    \"How does neuroscience influence AI architectures?\",\n]\n\n# Define LLM configurations\nllms = [llama_rag,mistral7b_rag,claude_rag]\n\n# Iterate through LLMs and test with observability\nfor llm in llms:\n    print(f\"Testing LLM: {llm.llm_model}\")\n\n    # Wrap RAG class with Trulens for observability\n    tru_rag = TruCustomApp(\n        llm,\n        app_id=\"SKO_OBSERVABILITY_1\",\n        app_version=llm.llm_model,\n        feedbacks=feedbacks,\n        metadata={\"model_name\": llm.llm_model}\n    )\n\n    # Test the pipeline\n    with tru_rag as recording:\n        for prompt in prompts:\n            try:\n                response = llm.query(prompt)\n                print(f\"Prompt: {prompt}\")\n                print(f\"Response: {response}\\n\")\n            except Exception as e:\n                print(f\"Error processing prompt: {prompt}\\nError: {str(e)}\\n\")\n                print(f\"Failed prompt:\\n{prompt}\\n{'-' * 50}\")\n            print(\"    \\n\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "71bad416-4100-469e-892e-740c6c13787e",
   "metadata": {
    "language": "python",
    "name": "ViewLLMLeaderboardd",
    "resultHeight": 146,
    "collapsed": false
   },
   "outputs": [],
   "source": "leaderboard = tru_session.get_leaderboard()\nleaderboard",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ad0d40c5-249b-4989-8cc3-9ec329c9c90d",
   "metadata": {
    "language": "python",
    "name": "AIObsApplication",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Define image in a stage and read the file\nimage=session.file.get_stream(\"@SKO.HOP.RAG/AIObsApp.jpg\", decompress=False).read() \n\n# Display the image\nst.image(image, width=800)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3dc402ee-ac04-41ef-9b64-b623b4a66d93",
   "metadata": {
    "language": "python",
    "name": "Summary",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "image1=session.file.get_stream(\"@SKO.HOP.RAG/Anthropic.jpg\", decompress=False).read() \nst.image(image1, width=800)\nimage2=session.file.get_stream(\"@SKO.HOP.RAG/Summary2.jpg\", decompress=False).read() \nst.image(image2, width=800)",
   "execution_count": null
  }
 ]
}